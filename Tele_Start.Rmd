---
title: "Group 1 Project 1: Telemarketing"
author: "Aakash Bharat, Maegan DeSmet, Ishan Goel, Doyeon Kim, Raamiz Qureshi"
date: "10/22/2022"
output:
  html_document:
    toc: true
    theme: united
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))

str(tele_norm)
```


```{r}
library(neuralnet)
library(class)
library(caret)
library(gmodels)
```

# Step 1: K-Means Clustering
```{r}
# custom normalization function

kmeans_tele = telemm # Make a copy

kmeans_tele$y = NULL # Remove y column

kmeans_tele_scaled = as.data.frame(lapply(kmeans_tele, scale))

# Run K-Means Clusters
set.seed(0)
tele_clusters <- kmeans(kmeans_tele_scaled, 5)
```

## Look at K-Means Data

```{r}
str(tele_clusters)

tele_clusters$size
tele_clusters$centers
```

## Aggregate Data

```{r}
# telemm is the cleaned up tele data with all factors converted
# to dummies (e.g. y gets turned into yyes)
telemm$cluster = tele_clusters$cluster
tele$cluster = tele_clusters$cluster
tele_norm$cluster = tele_clusters$cluster
clusteredSuccessRates =
  aggregate(data = telemm, yyes ~ cluster, mean)
clusteredSuccessRates
tele_clusters$size

# Weighted average to verify total success rate is ~11%
weighted.mean(clusteredSuccessRates$yyes, tele_clusters$size)
```

## Create Train and Test Data

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 10000)
# Depending on R-version and computer, different rows may be selected.
# If that happens, results are different.

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train_norm <-
  tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_test_norm <-
  tele_norm[test_set, -match("yyes",names(tele_norm))]

tele_train_reg = tele[-test_set,]# -match("y",names(tele))]
tele_test_reg = tele[test_set,  -match("y",names(tele))]

#Now the response (aka Labels) - only the yyes column
tele_train_labels_norm <- tele_norm[-test_set, "yyes"]
tele_test_labels_norm <- tele_norm[test_set, "yyes"]

tele_train_labels_reg <- tele[-test_set, "y"]
tele_test_labels_reg <- tele[test_set, "y"]
```


# Step 2: Logarithmic, KNN, and ANN

## Separate Clusters

```{r}
# Get clusters 1, 3, and 4 with logical indexing
tc1_train_norm = tele_train_norm[tele_train_norm$cluster == 1,]
tc3_train_norm = tele_train_norm[tele_train_norm$cluster == 3,]
tc4_train_norm = tele_train_norm[tele_train_norm$cluster == 4,]

tc1_train_reg = tele_train_reg[tele_train_reg$cluster == 1,]
tc3_train_reg = tele_train_reg[tele_train_reg$cluster == 3,]
tc4_train_reg = tele_train_reg[tele_train_reg$cluster == 4,]

tc1_test_norm = tele_test_norm[tele_test_norm$cluster == 1,]
tc3_test_norm = tele_test_norm[tele_test_norm$cluster == 3,]
tc4_test_norm = tele_test_norm[tele_test_norm$cluster == 4,]

tc1_test_reg = tele_test_reg[tele_test_reg$cluster == 1,]
tc3_test_reg = tele_test_reg[tele_test_reg$cluster == 3,]
tc4_test_reg = tele_test_reg[tele_test_reg$cluster == 4,]

tc1_train_labels_norm =
  tele_train_labels_norm[tele_train_norm$cluster == 1]
tc3_train_labels_norm =
  tele_train_labels_norm[tele_train_norm$cluster == 3]
tc4_train_labels_norm =
  tele_train_labels_norm[tele_train_norm$cluster == 4]

tc1_test_labels_norm =
  tele_test_labels_norm[tele_test_norm$cluster == 1]
tc3_test_labels_norm =
  tele_test_labels_norm[tele_test_norm$cluster == 3]
tc4_test_labels_norm =
  tele_test_labels_norm[tele_test_norm$cluster == 4]

# Clusters 2 and 5 are already above 17%
```


## Cluster 1

### Logistic
```{r}

tc1_log = glm(formula = y ~ .,
              data = tc1_train_reg,
              family = "binomial")

tc1_log_model = predict.glm(tc1_log,
              newdata = tc1_test_reg,
              type = "response")
tc1_log_model = ifelse(tc1_log_model > 0.5, 1, 0)

```

### K-Nearest Neighbors
```{r}
tc1_knn_model = knn(train = tc1_train_norm,
                    test = tc1_test_norm,
                      cl = tc1_train_labels_norm, k = 101)

```

### Artificial Neural Net
```{r, cache = TRUE}
# Put back yyes column in copy of tc1_train_norm
tc1_train_with_yyes = tc1_train_norm
tc1_train_with_yyes$yyes = as.numeric(tc1_train_reg$y) - 1

tc1_ann = neuralnet(formula = yyes ~ .,
                        data = tc1_train_with_yyes,
                        hidden = 3)
```


```{r}
tc1_ann_model = predict(tc1_ann, newdata = tc1_test_norm,
                        type = "response")

tc1_ann_model = ifelse(tc1_ann_model > 0.5, 1, 0)
```

### Combine Models

```{r}
# Add 3 models together to count "votes" for Yes
tc1_knn_model = as.numeric(tc1_knn_model)
tc1_combine_model = tc1_ann_model + tc1_knn_model + tc1_log_model
tc1_combine_model = ifelse(tc1_combine_model >= 2, 1, 0)
```

## Cluster 3

### Logistic
```{r}

#removed job since it doesn't have more than one value
tc3_log = glm(formula = y ~ age + marital + education + default +
                housing + loan + contact + month + day_of_week +
                campaign + previous + poutcome + emp.var.rate +
                cons.price.idx + cons.conf.idx + euribor3m +
                nr.employed + pdaysdummy + cluster,
                data = tc3_train_reg,
                family = "binomial")

tc3_log_model = predict.glm(tc3_log,
              newdata = tc3_test_reg,
              type = "response")
tc3_log_model = ifelse(tc3_log_model > 0.5, 1, 0)

```

### K-Nearest Neighbors
```{r}
tc3_knn_model = knn(train = tc3_train_norm,
                    test = tc3_test_norm,
                      cl = tc3_train_labels_norm, k = 53)

```

### Artificial Neural Net
```{r, cache = TRUE}
# Put back yyes column in copy of tc3_train_norm
tc3_train_with_yyes = tc3_train_norm
tc3_train_with_yyes$yyes = as.numeric(tc3_train_reg$y) - 1

tc3_ann = neuralnet(formula = yyes ~ .,
                        data = tc3_train_with_yyes,
                        hidden = 3, stepmax = 1e+05,
                        threshold = 0.1)
```


```{r}
tc3_ann_model = predict(tc3_ann, newdata = tc3_test_norm,
                        type = "response")

tc3_ann_model = ifelse(tc3_ann_model > 0.5, 1, 0)
```

### Combine Models

```{r}
# Add 3 models together to count "votes" for Yes
tc3_knn_model = as.numeric(tc3_knn_model)
tc3_combine_model = tc3_ann_model + tc3_knn_model + tc3_log_model
tc3_combine_model = ifelse(tc3_combine_model >= 2, 1, 0)
```

## Cluster 4

### Logistic
```{r}

#removed poutcome since it doesn't have more than one value
tc4_log = glm(formula = y ~ age + job + marital + education +
                default + housing + loan + contact + month +
                day_of_week + campaign + previous + emp.var.rate +
                cons.price.idx + cons.conf.idx + euribor3m +
                nr.employed + pdaysdummy + cluster,
                data = tc4_train_reg,
                family = "binomial")

tc4_log_model = predict.glm(tc4_log,
              newdata = tc4_test_reg,
              type = "response")
tc4_log_model = ifelse(tc4_log_model > 0.5, 1, 0)

```

### K-Nearest Neighbors
```{r}
tc4_knn_model = knn(train = tc4_train_norm,
                    test = tc4_test_norm,
                      cl = tc4_train_labels_norm, k = 47)

```

### Artificial Neural Net
```{r, cache = TRUE}
# Put back yyes column in copy of tc4_train_norm
tc4_train_with_yyes = tc4_train_norm
tc4_train_with_yyes$yyes = as.numeric(tc4_train_reg$y) - 1

# Take small subset of data for training
set.seed(0)
subsetIndices = sample(1:nrow(tc4_train_with_yyes), 7000)

tc4_train_with_yyes = tc4_train_with_yyes[subsetIndices,]

tc4_ann = neuralnet(formula = yyes ~ .,
                        data = tc4_train_with_yyes,
                        hidden = 3)
```


```{r}
tc4_ann_model = predict(tc4_ann, newdata = tc4_test_norm,
                        type = "response")

tc4_ann_model = ifelse(tc4_ann_model > 0.5, 1, 0)
```

### Combine Models

```{r}
# Add 3 models together to count "votes" for Yes
tc4_knn_model = as.numeric(tc4_knn_model)
tc4_combine_model = tc4_ann_model + tc4_knn_model + tc4_log_model
tc4_combine_model = ifelse(tc4_combine_model >= 2, 1, 0)
```

# Step 3: Combine Everything

```{r}
# final model is first a copy of tc1 combine model
# so that final model is at a least a list of some arbitrary length
final_model = tc1_combine_model
# Make full combine model filled with zeroes
for (i in 1:10000) {
  final_model[i] = 0
}
final_model = as.data.frame(final_model)
counter1 = 1
counter3 = 1
counter4 = 1
tc1df = as.data.frame(tc1_combine_model)
tc3df = as.data.frame(tc3_combine_model)
tc4df = as.data.frame(tc4_combine_model)
for (i in 1:nrow(final_model)) {
  # Call everyone in clusters 2 and 5
  if (tele_test_reg$cluster[i] == 2 |
      tele_test_reg$cluster[i] == 5) {
    final_model[i] = 1
  }
  # Use the model predictions for remaining clusters
  else if (tele_test_reg$cluster[i] == 1) {
    final_model[i] = tc1df$V1[counter1]
    counter1 = counter1 + 1
  }
  else if (tele_test_reg$cluster[i] == 3) {
    final_model[i] = tc3df$V1[counter3]
    counter3 = counter3 + 1
  }
  else if (tele_test_reg$cluster[i] == 4) {
    final_model[i] = tc4df$V1[counter4]
    counter4 = counter4 + 1
  }
}

fm = as.matrix(final_model[1,])
fm = as.data.frame(fm)
fm = t(fm)
fm = as.data.frame(fm)
fm$`1` = as.factor(fm$`1`)

CrossTable(x = tele_test_labels_norm, y = fm$`1`,
           prop.chisq=FALSE)
confusionMatrix(fm$`1`,
                as.factor(tele_test_labels_norm))
```

# Step 4: Conclusion

## Overview

* Cost Data: Each call costs $1 to make inclusive of all direct and indirect costs and overheads.

* Revenue Data: Each successful call leads to $6 in revenue. Unsuccessful calls of course result in no revenue. There is no other cost except the call center cost.

* Goal: Ensure the call center is not generating a loss. Call centers from a business perspective are not meant to be profitable; they just need to break even as a bare minimum.

## Cost-Benefit Analysis

### Current

* Calls = 41,188
* Cost per call = $1
* Total cost = $41,188
* Successful calls = 4,640
* Total revenue = 4,640 * $6 = $27,840
* Total loss = $27,840 - $41,188 = ($13,348)
* Current Success Rate = 4,640 calls / 41,188 calls * 100% = 11.26542%
* Break Even Success Rate = $1 / $6 * 100% = 16.67% = ~17% 

### Recommendation

* Calls = 1,495
* Cost per call = $1
* Total cost = $1,495
* Successful calls = 312
* Total revenue = 312 * $6 = $1,872
* Total profit = $1,872 -  $1,495 = $377
* New success rate = 312 calls / 1,495 class * 100% = 20.8696%  
  

Thus, we recommend the new model since it has a 20.8696% success rate, which is greater than the break-even threshold of 17%. Our recommendation will generate $377 of profit, reaching the goal of not operating at a loss. This model divides the data into 5 clusters, after which it generates a logistic regression model, a kNN model, and ANN model for all underperforming clusters. Of those three models, for each cluster, a combined model is created using a majority-voting scheme. In the end, if a data point belongs to a cluster that is over a 17% success rate (cluster 2 has ~20%; cluster 5 has ~30%), then it automatically recommends calling the person associated with that data point. Otherwise, it uses the combined model’s recommendation associated with the cluster to which that data point belongs.

## Choosing the Best Model

From a managerial perspective, it is best to use the combined model. The combined model uses more information to make a prediction since it is based on a majority prediction of the KNN, logistic regression, and ANN models. This allows managers to improve the allocation of resources by only calling customers that two of the three models have predicted will make a purchase. Furthermore, each model when used independently has their own strengths and weaknesses. The logistic model is great at identifying strong correlations between various variables, but they need not be a causal link. In addition, the various interaction terms can be difficult to pinpoint with too many terms leading to overfitting, and the amount of variables to take into account can be difficult for the designer of the logistic model. The KNN model is simple and useful, but it first assumes that nearby data points have similar outcomes, and second threads a fine line between overfitting and underfitting. The ANN model is accurate, but it comes at the cost of complexity, time, and difficulty in tuning. As a whole, both the ANN and logistic models have an extra step where the designer has to determine a threshold upon which a value corresponds to 1 or 0, which can involve tedious tuning. Since each of these models have their own benefits and drawbacks, a combined model incorporates the strengths of all three models to create an overall optimal prediction.
